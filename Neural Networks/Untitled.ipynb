{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy\n",
    "import timeit\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdata_CSV(filename):\n",
    "    return np.genfromtxt(filename,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_nk = getdata_CSV(\"cleveland.csv\")\n",
    "x_train = data_nk[:,0:5][:200]\n",
    "y_train = data_nk[:,13][:200]\n",
    "\n",
    "x_train = normalize(x_train, axis=0, norm='l1')\n",
    "\n",
    "x_test = data_nk[:,0:5][200:]\n",
    "y_test = data_nk[:,13][200:]\n",
    "x_test = normalize(x_test, axis=0, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200L, 5L)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_predict(theta,x):\n",
    "   \n",
    "    return scipy.special.expit(np.dot(theta, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_predict(theta,x):\n",
    "    nr=np.dot(theta,x)\n",
    "    probabilities = nr/np.sum(nr,axis=0)  \n",
    "        \n",
    "\n",
    "    return np.argmax(probabilities,axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Front with 2 layer to find Z and $\\hat Y$ with random W$_j$ and V$_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gradient Descent on W$_j$ and V$_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicator(x,y):\n",
    "    if x==y:\n",
    "        return y\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: 0\n",
      "j: 1\n",
      "j: 2\n",
      "j: 3\n",
      "j: 4\n",
      "j: 5\n",
      "j: 6\n",
      "j: 7\n",
      "j: 8\n",
      "j: 9\n",
      "j: 10\n",
      "j: 11\n",
      "j: 12\n",
      "j: 13\n",
      "j: 14\n",
      "j: 15\n",
      "j: 16\n",
      "j: 17\n",
      "j: 18\n",
      "j: 19\n",
      "j: 20\n",
      "j: 21\n",
      "j: 22\n",
      "j: 23\n",
      "j: 24\n",
      "j: 25\n",
      "j: 26\n",
      "j: 27\n",
      "j: 28\n",
      "j: 29\n",
      "j: 30\n",
      "j: 31\n",
      "j: 32\n",
      "j: 33\n",
      "j: 34\n",
      "j: 35\n",
      "j: 36\n",
      "j: 37\n",
      "j: 38\n",
      "j: 39\n",
      "j: 40\n",
      "j: 41\n",
      "j: 42\n",
      "j: 43\n",
      "j: 44\n",
      "j: 45\n",
      "j: 46\n",
      "j: 47\n",
      "j: 48\n",
      "j: 49\n",
      "j: 50\n",
      "j: 51\n",
      "j: 52\n",
      "j: 53\n",
      "j: 54\n",
      "j: 55\n",
      "j: 56\n",
      "j: 57\n",
      "j: 58\n",
      "j: 59\n",
      "j: 60\n",
      "j: 61\n",
      "j: 62\n",
      "j: 63\n",
      "j: 64\n",
      "j: 65\n",
      "j: 66\n",
      "j: 67\n",
      "j: 68\n",
      "j: 69\n",
      "j: 70\n",
      "j: 71\n",
      "j: 72\n",
      "j: 73\n",
      "j: 74\n",
      "j: 75\n",
      "j: 76\n",
      "j: 77\n",
      "j: 78\n",
      "j: 79\n",
      "j: 80\n",
      "j: 81\n",
      "j: 82\n",
      "j: 83\n",
      "j: 84\n",
      "j: 85\n",
      "j: 86\n",
      "j: 87\n",
      "j: 88\n",
      "j: 89\n",
      "j: 90\n",
      "j: 91\n",
      "j: 92\n",
      "j: 93\n",
      "j: 94\n",
      "j: 95\n",
      "j: 96\n",
      "j: 97\n",
      "j: 98\n",
      "j: 99\n",
      "j: 100\n",
      "j: 101\n",
      "j: 102\n",
      "j: 103\n",
      "j: 104\n",
      "j: 105\n",
      "j: 106\n",
      "j: 107\n",
      "j: 108\n",
      "j: 109\n",
      "j: 110\n",
      "j: 111\n",
      "j: 112\n",
      "j: 113\n",
      "j: 114\n",
      "j: 115\n",
      "j: 116\n",
      "j: 117\n",
      "j: 118\n",
      "j: 119\n",
      "j: 120\n",
      "j: 121\n",
      "j: 122\n",
      "j: 123\n",
      "j: 124\n",
      "j: 125\n",
      "j: 126\n",
      "j: 127\n",
      "j: 128\n",
      "j: 129\n",
      "j: 130\n",
      "j: 131\n",
      "j: 132\n",
      "j: 133\n",
      "j: 134\n",
      "j: 135\n",
      "j: 136\n",
      "j: 137\n",
      "j: 138\n",
      "j: 139\n",
      "j: 140\n",
      "j: 141\n",
      "j: 142\n",
      "j: 143\n",
      "j: 144\n",
      "j: 145\n",
      "j: 146\n",
      "j: 147\n",
      "j: 148\n",
      "j: 149\n",
      "j: 150\n",
      "j: 151\n",
      "j: 152\n",
      "j: 153\n",
      "j: 154\n",
      "j: 155\n",
      "j: 156\n",
      "j: 157\n",
      "j: 158\n",
      "j: 159\n",
      "j: 160\n",
      "j: 161\n",
      "j: 162\n",
      "j: 163\n",
      "j: 164\n",
      "j: 165\n",
      "j: 166\n",
      "j: 167\n",
      "j: 168\n",
      "j: 169\n",
      "j: 170\n",
      "j: 171\n",
      "j: 172\n",
      "j: 173\n",
      "j: 174\n",
      "j: 175\n",
      "j: 176\n",
      "j: 177\n",
      "j: 178\n",
      "j: 179\n",
      "j: 180\n",
      "j: 181\n",
      "j: 182\n",
      "j: 183\n",
      "j: 184\n",
      "j: 185\n",
      "j: 186\n",
      "j: 187\n",
      "j: 188\n",
      "j: 189\n",
      "j: 190\n",
      "j: 191\n",
      "j: 192\n",
      "j: 193\n",
      "j: 194\n",
      "j: 195\n",
      "j: 196\n",
      "j: 197\n",
      "j: 198\n",
      "j: 199\n",
      "j: 200\n",
      "j: 201\n",
      "j: 202\n",
      "j: 203\n",
      "j: 204\n",
      "j: 205\n",
      "j: 206\n",
      "j: 207\n",
      "j: 208\n",
      "j: 209\n",
      "j: 210\n",
      "j: 211\n",
      "j: 212\n",
      "j: 213\n",
      "j: 214\n",
      "j: 215\n",
      "j: 216\n",
      "j: 217\n",
      "j: 218\n",
      "j: 219\n",
      "j: 220\n",
      "j: 221\n",
      "j: 222\n",
      "j: 223\n",
      "j: 224\n",
      "j: 225\n",
      "j: 226\n",
      "j: 227\n",
      "j: 228\n",
      "j: 229\n",
      "j: 230\n",
      "j: 231\n",
      "j: 232\n",
      "j: 233\n",
      "j: 234\n",
      "j: 235\n",
      "j: 236\n",
      "j: 237\n",
      "j: 238\n",
      "j: 239\n",
      "j: 240\n",
      "j: 241\n",
      "j: 242\n",
      "j: 243\n",
      "j: 244\n",
      "j: 245\n",
      "j: 246\n",
      "j: 247\n",
      "j: 248\n",
      "j: 249\n",
      "j: 250\n",
      "j: 251\n",
      "j: 252\n",
      "j: 253\n",
      "j: 254\n",
      "j: 255\n",
      "j: 256\n",
      "j: 257\n",
      "j: 258\n",
      "j: 259\n",
      "j: 260\n",
      "j: 261\n",
      "j: 262\n",
      "j: 263\n",
      "j: 264\n",
      "j: 265\n",
      "j: 266\n",
      "j: 267\n",
      "j: 268\n",
      "j: 269\n",
      "j: 270\n",
      "j: 271\n",
      "j: 272\n",
      "j: 273\n",
      "j: 274\n",
      "j: 275\n",
      "j: 276\n",
      "j: 277\n",
      "j: 278\n",
      "j: 279\n",
      "j: 280\n",
      "j: 281\n",
      "j: 282\n",
      "j: 283\n",
      "j: 284\n",
      "j: 285\n",
      "j: 286\n",
      "j: 287\n",
      "j: 288\n",
      "j: 289\n",
      "j: 290\n",
      "j: 291\n",
      "j: 292\n",
      "j: 293\n",
      "j: 294\n",
      "j: 295\n",
      "j: 296\n",
      "j: 297\n",
      "j: 298\n",
      "j: 299\n",
      "j: 300\n",
      "j: 301\n",
      "j: 302\n",
      "j: 303\n",
      "j: 304\n",
      "j: 305\n",
      "j: 306\n",
      "j: 307\n",
      "j: 308\n",
      "j: 309\n",
      "j: 310\n",
      "j: 311\n",
      "j: 312\n",
      "j: 313\n",
      "j: 314\n",
      "j: 315\n",
      "j: 316\n",
      "j: 317\n",
      "j: 318\n",
      "j: 319\n",
      "j: 320\n",
      "j: 321\n",
      "j: 322\n",
      "j: 323\n",
      "j: 324\n",
      "j: 325\n",
      "j: 326\n",
      "j: 327\n",
      "j: 328\n",
      "j: 329\n",
      "j: 330\n",
      "j: 331\n",
      "j: 332\n",
      "j: 333\n",
      "j: 334\n",
      "j: 335\n",
      "j: 336\n",
      "j: 337\n",
      "j: 338\n",
      "j: 339\n",
      "j: 340\n",
      "j: 341\n",
      "j: 342\n",
      "j: 343\n",
      "j: 344\n",
      "j: 345\n",
      "j: 346\n",
      "j: 347\n",
      "j: 348\n",
      "j: 349\n",
      "j: 350\n",
      "j: 351\n",
      "j: 352\n",
      "j: 353\n",
      "j: 354\n",
      "j: 355\n",
      "j: 356\n",
      "j: 357\n",
      "j: 358\n",
      "j: 359\n",
      "j: 360\n",
      "j: 361\n",
      "j: 362\n",
      "j: 363\n",
      "j: 364\n",
      "j: 365\n",
      "j: 366\n",
      "j: 367\n",
      "j: 368\n",
      "j: 369\n",
      "j: 370\n",
      "j: 371\n",
      "j: 372\n",
      "j: 373\n",
      "j: 374\n",
      "j: 375\n",
      "j: 376\n",
      "j: 377\n",
      "j: 378\n",
      "j: 379\n",
      "j: 380\n",
      "j: 381\n",
      "j: 382\n",
      "j: 383\n",
      "j: 384\n",
      "j: 385\n",
      "j: 386\n",
      "j: 387\n",
      "j: 388\n",
      "j: 389\n",
      "j: 390\n",
      "j: 391\n",
      "j: 392\n",
      "j: 393\n",
      "j: 394\n",
      "j: 395\n",
      "j: 396\n",
      "j: 397\n",
      "j: 398\n",
      "j: 399\n",
      "j: 400\n",
      "j: 401\n",
      "j: 402\n",
      "j: 403\n",
      "j: 404\n",
      "j: 405\n",
      "j: 406\n",
      "j: 407\n",
      "j: 408\n",
      "j: 409\n",
      "j: 410\n",
      "j: 411\n",
      "j: 412\n",
      "j: 413\n",
      "j: 414\n",
      "j: 415\n",
      "j: 416\n",
      "j: 417\n",
      "j: 418\n",
      "j: 419\n",
      "j: 420\n",
      "j: 421\n",
      "j: 422\n",
      "j: 423\n",
      "j: 424\n",
      "j: 425\n",
      "j: 426\n",
      "j: 427\n",
      "j: 428\n",
      "j: 429\n",
      "j: 430\n",
      "j: 431\n",
      "j: 432\n",
      "j: 433\n",
      "j: 434\n",
      "j: 435\n",
      "j: 436\n",
      "j: 437\n",
      "j: 438\n",
      "j: 439\n",
      "j: 440\n",
      "j: 441\n",
      "j: 442\n",
      "j: 443\n",
      "j: 444\n",
      "j: 445\n",
      "j: 446\n",
      "j: 447\n",
      "j: 448\n",
      "j: 449\n",
      "j: 450\n",
      "j: 451\n",
      "j: 452\n",
      "j: 453\n",
      "j: 454\n",
      "j: 455\n",
      "j: 456\n",
      "j: 457\n",
      "j: 458\n",
      "j: 459\n",
      "j: 460\n",
      "j: 461\n",
      "j: 462\n",
      "j: 463\n",
      "j: 464\n",
      "j: 465\n",
      "j: 466\n",
      "j: 467\n",
      "j: 468\n",
      "j: 469\n",
      "j: 470\n",
      "j: 471\n",
      "j: 472\n",
      "j: 473\n",
      "j: 474\n",
      "j: 475\n",
      "j: 476\n",
      "j: 477\n",
      "j: 478\n",
      "j: 479\n",
      "j: 480\n",
      "j: 481\n",
      "j: 482\n",
      "j: 483\n",
      "j: 484\n",
      "j: 485\n",
      "j: 486\n",
      "j: 487\n",
      "j: 488\n",
      "j: 489\n",
      "j: 490\n",
      "j: 491\n",
      "j: 492\n",
      "j: 493\n",
      "j: 494\n",
      "j: 495\n",
      "j: 496\n",
      "j: 497\n",
      "j: 498\n",
      "j: 499\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(x,y, labels, h,learning_rate,maxIterations=10):\n",
    "    #iterative solution\n",
    "    iterations = 0\n",
    "    k = len(labels)\n",
    "    m,n = np.shape(x)\n",
    "\n",
    "    v = np.random.rand(k,h)/100\n",
    "    w = np.random.rand(h,n)/100\n",
    "    \n",
    "    \n",
    "    beta = 0.2\n",
    "  \n",
    "    z = np.transpose(sigmoid_predict(w,np.transpose(x)))\n",
    "    y_cap= softmax_predict(v,z.T)\n",
    " \n",
    "        \n",
    "        \n",
    "    while (iterations < maxIterations):\n",
    "        \n",
    "        \n",
    "        #update v\n",
    "        for j in range(len(labels)):\n",
    "            deltavj = 0\n",
    "            for i in range(x.shape[0]):\n",
    "                y_i = indicator(y[i],j)\n",
    "                if y_i!=0:\n",
    "                    deltavj += ((y_cap[i] - y_i) * z[i])\n",
    "            v[j] -= learning_rate * deltavj\n",
    "            \n",
    "            \n",
    "        #update w\n",
    "        \n",
    "        for j in range(h):\n",
    "            s = 0\n",
    "            for i in range(x.shape[0]):\n",
    "                deltawj = 0\n",
    "                \n",
    "                for l in range(len(labels)):\n",
    "                    y_i = indicator(y[i],j)\n",
    "                    if y_i !=0:\n",
    "                        deltawj += ((y_cap[i]  - indicator(y[i],l)) * v[l,j])\n",
    "                s += deltawj * z[i,j] * (1-z[i,j]) * x[i] \n",
    "                prev = w[j]\n",
    "                \n",
    "            w[j] -=  - learning_rate * s + beta*(w[j]-prev) #momentum\n",
    "        print \"j:\",iterations\n",
    "            \n",
    "        iterations += 1\n",
    "    \n",
    "        \n",
    "    return v,w\n",
    "\n",
    "\n",
    "k = np.unique(y_test)\n",
    "n = len(x_train)\n",
    "h=(n+len(k))/2\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "v,w = gradient_descent(x_train,y_train, labels, h,learning_rate=0.0002,maxIterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Predict with new  W$_j$ and V$_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "z= sigmoid_predict(w,x_test.T)\n",
    "y_predict =  softmax_predict(v,z)\n",
    "print y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
       "      f_stable=0.001,\n",
       "      hidden0=<sknn.nn.Layer `Sigmoid`: units=102, name=u'hidden0', frozen=False>,\n",
       "      layers=[<sknn.nn.Layer `Sigmoid`: units=102, name=u'hidden0', frozen=False>, <sknn.nn.Layer `Softmax`: units=5L, name=u'output', frozen=False>],\n",
       "      learning_momentum=0.9, learning_rate=0.2, learning_rule='momentum',\n",
       "      loss_type=None, n_iter=5, n_stable=10,\n",
       "      output=<sknn.nn.Layer `Softmax`: units=5L, name=u'output', frozen=False>,\n",
       "      random_state=None, regularize=None, valid_set=None, valid_size=0.0,\n",
       "      verbose=None, warning=None, weight_decay=None, weights=None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Sigmoid\", units=102),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.2,\n",
    "    learning_rule=\"momentum\",\n",
    "    n_iter=5)\n",
    "nn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameters(weights=array([[-0.1074548 , -0.05992579,  0.23340656,  0.03892696, -0.09935599],\n",
       "       [ 0.11733401, -0.02279962, -0.19513408,  0.06238991,  0.00743613],\n",
       "       [-0.02347598,  0.0070815 , -0.19012055,  0.01253562, -0.0868135 ],\n",
       "       [ 0.25109939, -0.0616901 ,  0.18965255,  0.07994162, -0.06958761],\n",
       "       [ 0.09688634, -0.06456256, -0.10771801,  0.08029196,  0.03596739],\n",
       "       [-0.18411495, -0.1303738 ,  0.00800007,  0.15596191,  0.09394592],\n",
       "       [-0.09909391, -0.12247139, -0.1122262 ,  0.10244907, -0.00798247],\n",
       "       [-0.13572111, -0.1850946 ,  0.01627393,  0.0181111 ,  0.09925428],\n",
       "       [-0.16612861,  0.21620105,  0.08273941,  0.15937434, -0.18034369],\n",
       "       [ 0.14552328, -0.12991901,  0.04309352,  0.1926919 , -0.08738149],\n",
       "       [-0.0936734 , -0.13998468, -0.19875058, -0.22874516,  0.04082334],\n",
       "       [ 0.22491462,  0.06694286, -0.12029858,  0.02731105, -0.10482893],\n",
       "       [ 0.10064644,  0.01041091, -0.13406464,  0.18773968, -0.22709764],\n",
       "       [-0.05126141, -0.07956373,  0.2460313 , -0.24605052, -0.21241328],\n",
       "       [-0.14722884,  0.0739185 , -0.12358153, -0.21018591, -0.02854023],\n",
       "       [ 0.145618  , -0.06789772, -0.15793241,  0.14957543, -0.10791298],\n",
       "       [ 0.21383551,  0.17830064,  0.2287481 ,  0.17347242,  0.08106794],\n",
       "       [ 0.22152048, -0.09868907,  0.13150478,  0.13281233,  0.1435856 ],\n",
       "       [-0.06758371, -0.03811152,  0.12631134,  0.05001848, -0.15492578],\n",
       "       [ 0.2206743 ,  0.14333187,  0.22292481,  0.0827574 ,  0.00178093],\n",
       "       [ 0.02075505,  0.21138403,  0.24057473, -0.11352875, -0.23989814],\n",
       "       [-0.09581865,  0.08638356,  0.05909869,  0.04798254,  0.14060943],\n",
       "       [ 0.12608836, -0.05061076,  0.18818899,  0.10783756, -0.24746926],\n",
       "       [ 0.16794444,  0.18194004,  0.23211035,  0.10339434, -0.12381575],\n",
       "       [-0.17002245, -0.03233362, -0.17460694,  0.08041171,  0.05557802],\n",
       "       [ 0.04730706, -0.11602206, -0.05546016,  0.08822259, -0.01052508],\n",
       "       [ 0.00656738, -0.24362495,  0.1046714 ,  0.02066644, -0.05593985],\n",
       "       [-0.0737107 ,  0.17927074, -0.15554868,  0.18218978,  0.14857592],\n",
       "       [-0.13614136,  0.06501399, -0.17676243, -0.15930935,  0.00902338],\n",
       "       [ 0.16995351,  0.19020237, -0.19090454, -0.01615307,  0.13049839],\n",
       "       [-0.05181623,  0.0480236 ,  0.02632647, -0.09522677, -0.17021359],\n",
       "       [ 0.04667248, -0.17152357, -0.17662199,  0.03940873,  0.0245773 ],\n",
       "       [ 0.01601534, -0.0324542 ,  0.14380943, -0.04999726, -0.10950778],\n",
       "       [-0.0601843 ,  0.11325449,  0.07333969,  0.20015297,  0.09814502],\n",
       "       [-0.09917652, -0.19861412,  0.22997002,  0.13027401, -0.15066739],\n",
       "       [-0.14502869,  0.15676253,  0.1562076 , -0.14450548, -0.10208891],\n",
       "       [ 0.00355196,  0.08861036, -0.15111463,  0.18983198, -0.19959908],\n",
       "       [ 0.08565963,  0.2248158 ,  0.05024133, -0.14162862, -0.13142668],\n",
       "       [-0.09263353, -0.08454026, -0.0638119 ,  0.05927851, -0.02444951],\n",
       "       [ 0.12957372,  0.09011351, -0.18130673, -0.18155921, -0.25640182],\n",
       "       [ 0.24374525,  0.1197942 , -0.07599879,  0.20509467, -0.0778016 ],\n",
       "       [-0.02938033, -0.14554575, -0.03828321, -0.03118203,  0.18764955],\n",
       "       [-0.03277802,  0.01438407,  0.22284129, -0.05942633,  0.19706535],\n",
       "       [ 0.02534956,  0.08184962,  0.05188136,  0.21153629, -0.07053315],\n",
       "       [ 0.07296022,  0.19009009, -0.04145471,  0.2053852 , -0.04518966],\n",
       "       [ 0.02036739,  0.0710664 ,  0.1134617 , -0.07297395, -0.18091253],\n",
       "       [ 0.01305625, -0.16015559, -0.12570008, -0.16362084, -0.04046589],\n",
       "       [-0.19120377,  0.0039693 , -0.17558518, -0.1522598 , -0.04410803],\n",
       "       [ 0.13300867,  0.19969768,  0.14261554,  0.19954644, -0.07066339],\n",
       "       [ 0.17766387, -0.06278577, -0.1741263 ,  0.07036377, -0.12807385],\n",
       "       [ 0.24297856, -0.21765102, -0.21594925,  0.18787947, -0.01872374],\n",
       "       [-0.19203296, -0.12996971,  0.01013574, -0.05450817,  0.17944899],\n",
       "       [-0.05054995, -0.03386986,  0.0790656 ,  0.0684728 , -0.16299925],\n",
       "       [ 0.06253089, -0.15663492, -0.19611516,  0.1530507 , -0.14196281],\n",
       "       [ 0.23946634, -0.01945217,  0.09192216,  0.00979356, -0.04863526],\n",
       "       [ 0.02345987,  0.21269622, -0.00315827, -0.14355162,  0.08614599],\n",
       "       [ 0.26955537,  0.20948777,  0.02967279, -0.0427189 , -0.20328481],\n",
       "       [-0.04670807, -0.1295306 ,  0.15837284, -0.21953632, -0.0682991 ],\n",
       "       [-0.13038433,  0.20834822,  0.02779464, -0.19519486, -0.0475278 ],\n",
       "       [-0.04312572, -0.16883778,  0.16274411,  0.15991163, -0.04701049],\n",
       "       [-0.17340835,  0.04977602, -0.13414409, -0.20516472, -0.17984708],\n",
       "       [-0.10105039, -0.04073789, -0.17046234,  0.09330687,  0.17082314],\n",
       "       [-0.18752642, -0.23216783, -0.20434109, -0.1981109 ,  0.095045  ],\n",
       "       [-0.12760676,  0.19836328, -0.18624498,  0.03731717, -0.24020844],\n",
       "       [-0.12239877, -0.11180373, -0.05027615, -0.10171295,  0.15491428],\n",
       "       [ 0.11673105, -0.18531514, -0.1545691 , -0.07248443,  0.12343351],\n",
       "       [ 0.21782848,  0.03912454, -0.07367385, -0.12694857, -0.15450437],\n",
       "       [-0.00532001, -0.15676929, -0.09763068, -0.16422639,  0.07543543],\n",
       "       [ 0.22073723,  0.12470163, -0.19136716, -0.13302405, -0.16332956],\n",
       "       [ 0.12840625, -0.00822221,  0.02695994, -0.00092033,  0.18752467],\n",
       "       [ 0.1816772 ,  0.02824641, -0.16028947, -0.23355921, -0.088622  ],\n",
       "       [ 0.14290847,  0.12790351, -0.11713603, -0.22518448,  0.18036713],\n",
       "       [ 0.22263084, -0.2417246 , -0.07111623, -0.14925851, -0.1321514 ],\n",
       "       [ 0.00606148,  0.02407785,  0.11009418, -0.11004089,  0.08502552],\n",
       "       [-0.16710042, -0.05396178, -0.13238382, -0.16820019,  0.20639004],\n",
       "       [ 0.00056099, -0.09234014,  0.20795849,  0.16104933, -0.07051331],\n",
       "       [-0.06250813,  0.18093069, -0.19784347, -0.030559  ,  0.13808343],\n",
       "       [-0.01955102,  0.01228169,  0.02364698, -0.10746314,  0.04779895],\n",
       "       [ 0.26871597, -0.18126887, -0.17314918, -0.24033916, -0.08024359],\n",
       "       [ 0.06844822,  0.18563229,  0.01922291, -0.23773817, -0.06136165],\n",
       "       [-0.10984618,  0.14412926,  0.2447183 ,  0.13547033, -0.14841168],\n",
       "       [ 0.08665539, -0.0818141 , -0.15066523,  0.16758471, -0.06534068],\n",
       "       [ 0.02657071,  0.11482591, -0.08618796, -0.11154858, -0.23244479],\n",
       "       [-0.1466309 , -0.23708944,  0.0004065 , -0.09386108,  0.10755945],\n",
       "       [-0.19737987,  0.17905772, -0.18387229, -0.04797382,  0.18481895],\n",
       "       [-0.02232467,  0.07790224,  0.20665177,  0.09048985,  0.02404659],\n",
       "       [-0.18891458,  0.03748581, -0.14144756,  0.11894533, -0.10351481],\n",
       "       [ 0.06259058,  0.11391813, -0.04120056, -0.24598641, -0.14544136],\n",
       "       [-0.10283271, -0.02118185,  0.11252067, -0.10364301, -0.23545401],\n",
       "       [-0.00101395, -0.19100413,  0.08272292,  0.18859201, -0.10559131],\n",
       "       [ 0.22676881,  0.08421051,  0.1618802 , -0.18875041, -0.06607129],\n",
       "       [ 0.13576413,  0.12624013, -0.11983244,  0.07858619, -0.12872035],\n",
       "       [-0.0792248 , -0.24697215, -0.04087269, -0.23432779,  0.16417323],\n",
       "       [ 0.2328736 , -0.17213855,  0.00291187, -0.09814669,  0.19626691],\n",
       "       [ 0.12767905, -0.15634315, -0.08104084, -0.05279218,  0.18449991],\n",
       "       [-0.17210318,  0.06882008, -0.14767967, -0.13397525,  0.19171989],\n",
       "       [ 0.18854137,  0.15826618, -0.03450806,  0.04610188,  0.19766372],\n",
       "       [-0.11137384, -0.20905675,  0.14656082, -0.16965395, -0.04367148],\n",
       "       [ 0.10113489, -0.14281462, -0.20271176, -0.15899231, -0.06440026],\n",
       "       [-0.1161405 ,  0.14645513, -0.13559336,  0.1212928 , -0.08538458],\n",
       "       [-0.14320622, -0.17605639, -0.02809916,  0.13732265, -0.16608191],\n",
       "       [-0.03612849,  0.16129902,  0.11450619, -0.13633148,  0.05240597]]), biases=array([ 0.06499259, -0.021814  ,  0.037548  , -0.03149622, -0.04923037]), layer=u'output')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
