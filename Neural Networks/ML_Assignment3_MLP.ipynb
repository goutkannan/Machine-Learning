{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy\n",
    "import timeit\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing Iris Image Dataset \n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # we only take the first two features.\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = np.column_stack((X , Y))\n",
    "samples =np.array(random.sample(samples, len(samples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = samples[:100,0:3]\n",
    "y_train = samples[:100,4]\n",
    "x_test = samples[100:,0:3]\n",
    "y_test = samples[100:,4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Sigmoid\n",
    "$f(x) = 1/{1+\\exp^{-\\theta.T . x}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_predict(theta,x):\n",
    "    \"\"\" We find the sigmoid for a given set of theta and sample \"\"\"\n",
    "    return scipy.special.expit(np.dot(theta, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calculating the Softmax\n",
    "$ \\forall_j  \\arg\\max  (\\exp^{\\theta_J \\cdot x}/\\sum_{j=1}^{k} \\exp^{\\theta_J \\cdot x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(theta,x):\n",
    "    nr=np.exp((np.dot(theta,x)))\n",
    "    probabilities =(nr)/(np.sum(nr,axis=0))  \n",
    "        \n",
    "\n",
    "    return np.argmax(probabilities,axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicator(x,y):\n",
    "    if x==y:\n",
    "        return y\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gradient Descent on W$_j$ and V$_j$\n",
    "\n",
    "\n",
    "### Feed Front with 2 layer to find Z and $\\hat Y$ with random W$_j$ and V$_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x,y, labels, h,learning_rate,maxIterations=10):\n",
    "    #iterative solution\n",
    "    iterations = 0\n",
    "    k = len(labels)\n",
    "    m,n = np.shape(x)\n",
    "\n",
    "    v = np.random.rand(k,h)/100\n",
    "    w = np.random.rand(h,n)/100\n",
    "    \n",
    "    \n",
    "    beta = 0.2\n",
    "  \n",
    "    z = np.transpose(sigmoid_predict(w,np.transpose(x)))\n",
    "    y_cap= softmax_predict(v,z.T)\n",
    " \n",
    "        \n",
    "    \n",
    "    while (iterations < maxIterations):\n",
    "        ## Claculating  deltavj and then v    \n",
    "        for j in range(len(labels)):\n",
    "            deltavj = 0\n",
    "            for i in range(x.shape[0]):\n",
    "                y_i = indicator(y[i],j)\n",
    "                if y_i!=0:\n",
    "                    deltavj += ((y_cap[i] - y_i) * z[i])\n",
    "            v[j] -= learning_rate * deltavj\n",
    "            \n",
    "        ## Claculating  deltawj and then w    \n",
    "        for j in range(h):\n",
    "            s = 0\n",
    "            for i in range(x.shape[0]):\n",
    "                deltawj = 0\n",
    "                \n",
    "                for l in range(len(labels)):\n",
    "                    y_i = indicator(y[i],j)\n",
    "                    if y_i !=0:\n",
    "                        deltawj += ((y_cap[i]  - indicator(y[i],l)) * v[l,j])\n",
    "                s += deltawj * z[i,j] * (1-z[i,j]) * x[i] \n",
    "                prev = w[j]\n",
    "            # Adding the  momentum parameter    \n",
    "            w[j] -=  - learning_rate * s + beta*(w[j]-prev) \n",
    "      \n",
    "            \n",
    "        iterations += 1\n",
    "          \n",
    "    return v,w\n",
    "\n",
    "\n",
    "k = np.unique(y_test)\n",
    "n = len(x_train)\n",
    "h=(n+len(k))/2\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "v,w = gradient_descent(x_train,y_train, labels, h,learning_rate=0.002,maxIterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z= sigmoid_predict(w,x_test.T)\n",
    "y_predict =  softmax_predict(v,z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y,y_cap):\n",
    "    \"\"\" Computes the mean squared Error\"\"\"\n",
    "    errors = 0\n",
    "    for i in range(0,len(y)):        \n",
    "        errors = errors + (y_cap[i] -y[i])**2\n",
    "        \n",
    "    return errors.sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predict =  y_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(y_Cap,y,cls):\n",
    "    \"\"\" Using the Confusion matrix , the metric like Accuracy , Precision , Recall , F square measures are computed \"\"\"\n",
    "    a,b,c,d = confussion_matrix(y_Cap,y,cls)\n",
    "    r =0 \n",
    "    p= 0\n",
    "    if a+b+c+d!=0:\n",
    "        print \"Accuracy \\t:\",(a+d)/(a+b+c+d)\n",
    "    else: \n",
    "        print \"Can't Compute Accurancy\"\n",
    "    if c+d !=0:\n",
    "        \n",
    "        r = (d)/(c+d)\n",
    "        print \"Recall  \\t:\",r\n",
    "        print \"False Negative \\t:\",c/(c+d)\n",
    "    else: \n",
    "        print \"Can't Compute Recall and False Negative\"\n",
    "        r=0\n",
    "    if b+d!=0:    \n",
    "        p = d/(b+d)\n",
    "        print \"Precision \\t:\",p\n",
    "    else: \n",
    "        print \"Can't Compute Precision \"\n",
    "        p=0\n",
    "    if a+b!=0:\n",
    "        print \"False Positive \\t:\",b/(a+b)\n",
    "        print \"True Negative \\t:\",a/(a+b)\n",
    "    else:\n",
    "         print \"Can't Compute False Positive and True Negatice\"\n",
    "    if p+r !=0:\n",
    "        print \"F Square \\t:\",2*(p*r)/(p+r)\n",
    "    else:\n",
    "        print\"Can't Compute F Square\"\n",
    "    return p,r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion Matrix \n",
      "[[ 15.   4.]\n",
      " [ 12.  19.]]\n"
     ]
    }
   ],
   "source": [
    "def confussion_matrix(y_cap,y,cls=1):\n",
    "    \"\"\" Compute the confussion matrix for the given predicted and actual classes \"\"\"\n",
    "    a=0\n",
    "    b=0\n",
    "    c=0\n",
    "    d=0\n",
    "    for i in range(0,len(y)):        \n",
    "        if y[i]==cls and y_cap[i] ==cls:\n",
    "            a+=1\n",
    "        elif y[i]!=cls and y_cap[i] ==cls:\n",
    "            c+=1\n",
    "        elif y[i]==cls and y_cap[i] !=cls:\n",
    "            b+=1\n",
    "        elif y[i]!=cls and y_cap[i] !=cls:\n",
    "            d+=1\n",
    "    return float(a),float(b),float(c),float(d)\n",
    "\n",
    "\n",
    "a,b,c,d = confussion_matrix(y_predict, y_test,1)\n",
    "print \"Confussion Matrix \\n\", np.matrix([[a,b],[c,d]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Performance of Implemented MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 2 2 1 1 1 1 0 0 1 0 2 1 1 0 2 2 1 2 0 1 0 1 1 2 0 2 2 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 0 1 1 0 0]\n",
      "MSE using Impleneted MLP:  0.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print y_predict\n",
    "print \"MSE using Impleneted MLP: \", mean_squared_error(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a label  0.0 \n",
      "Accuracy \t: 0.7\n",
      "Recall  \t: 0.875\n",
      "False Negative \t: 0.125\n",
      "Precision \t: 0.717948717949\n",
      "False Positive \t: 0.611111111111\n",
      "True Negative \t: 0.388888888889\n",
      "F Square \t: 0.788732394366\n",
      "(0.717948717948718, 0.875)\n",
      "For a label  1.0 \n",
      "Accuracy \t: 0.68\n",
      "Recall  \t: 0.612903225806\n",
      "False Negative \t: 0.387096774194\n",
      "Precision \t: 0.826086956522\n",
      "False Positive \t: 0.210526315789\n",
      "True Negative \t: 0.789473684211\n",
      "F Square \t: 0.703703703704\n",
      "(0.8260869565217391, 0.6129032258064516)\n",
      "For a label  2.0 \n",
      "Accuracy \t: 0.82\n",
      "Recall  \t: 0.891891891892\n",
      "False Negative \t: 0.108108108108\n",
      "Precision \t: 0.868421052632\n",
      "False Positive \t: 0.384615384615\n",
      "True Negative \t: 0.615384615385\n",
      "F Square \t: 0.88\n",
      "(0.868421052631579, 0.8918918918918919)\n"
     ]
    }
   ],
   "source": [
    "for i in (np.unique(y_test)):\n",
    "    print \"For a label \" ,i ,'\\n',evaluate_performance(y_predict, y_test,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Performance of Scikit -Learn's  MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Sigmoid\", units=102),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.002,\n",
    "    learning_rule=\"momentum\",\n",
    "    n_iter=500)\n",
    "nn.fit(x_train, y_train)\n",
    "sK_y_predict = nn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a label  0.0 \n",
      "Accuracy \t: 1.0\n",
      "Recall  \t: 1.0\n",
      "False Negative \t: 0.0\n",
      "Precision \t: 1.0\n",
      "False Positive \t: 0.0\n",
      "True Negative \t: 1.0\n",
      "F Square \t: 1.0\n",
      "(1.0, 1.0)\n",
      "For a label  1.0 \n",
      "Accuracy \t: 0.98\n",
      "Recall  \t: 0.967741935484\n",
      "False Negative \t: 0.0322580645161\n",
      "Precision \t: 1.0\n",
      "False Positive \t: 0.0\n",
      "True Negative \t: 1.0\n",
      "F Square \t: 0.983606557377\n",
      "(1.0, 0.967741935483871)\n",
      "For a label  2.0 \n",
      "Accuracy \t: 0.98\n",
      "Recall  \t: 1.0\n",
      "False Negative \t: 0.0\n",
      "Precision \t: 0.973684210526\n",
      "False Positive \t: 0.0769230769231\n",
      "True Negative \t: 0.923076923077\n",
      "F Square \t: 0.986666666667\n",
      "(0.9736842105263158, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in (np.unique(y_test)):\n",
    "    print \"For a label \" ,i ,'\\n',evaluate_performance(sK_y_predict, y_test,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
